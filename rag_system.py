# -*- coding: utf-8 -*-
"""rag system

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1hYpY5yoIocZMwJVa3oCi7ufUbPkrE9FR

# Simple RAG Example with Weaviate and LangChain

## 1. Introduction

### 1.1 Plan and Objectives

This notebook demonstrates a simple implementation of the Retrieval-Augmented Generation (RAG) pattern. The goal is to build a question-answering system that leverages a vector database to provide context-aware answers from a Large Language Model (LLM).

The process involves:
1.  **Data Preparation**: Creating a small, factual dataset of scientific notes.
2.  **Environment Setup**: Preparing the Docker environment and installing dependencies.
3.  **Database Deployment**: Launching a Weaviate vector database instance using Docker.
4.  **Embeddings & Ingestion**: Generating vector embeddings for our data using Azure OpenAI and loading it into Weaviate.
5.  **RAG Experiment**: Executing a RAG pipeline:
    - Expanding a user's question.
    - Searching for relevant documents in Weaviate.
    - Generating a final answer using an LLM augmented with the retrieved documents.
6.  **Cleanup**: Removing the Docker container to free up system resources.

## 2. System Environment Preparation

This section contains helper functions to interact with the underlying operating system (Linux or Windows with WSL) to manage Docker containers and file paths.

### 2.1. WSL and Shell Command Helpers
"""

import platform
import subprocess
import os

# --- WSL Detection ---
system = platform.system()
USE_WSL = system == "Windows"
print(f"Operating System: {system}. Using WSL for Docker commands: {USE_WSL}")

# --- Shell Command Helpers ---
def run_wsl_command(command):
    """Executes a command inside WSL and returns the result."""
    result = subprocess.run(
        ["wsl", "-e", "bash", "-l", "-c", command],
        capture_output=True,
        text=True,
        encoding="utf-8",
        errors="replace"
    )
    return {
        "returncode": result.returncode,
        "stdout": result.stdout.strip(),
        "stderr": result.stderr.strip(),
        "success": result.returncode == 0
    }

def run_linux_command(command):
    """Executes a command in a standard Linux/macOS shell."""
    result = subprocess.run(
        command,
        shell=True,
        capture_output=True,
        text=True,
        encoding="utf-8",
        errors="replace"
    )
    return {
        "returncode": result.returncode,
        "stdout": result.stdout.strip(),
        "stderr": result.stderr.strip(),
        "success": result.returncode == 0
    }

def run_shell_command(command):
    """Universal function to run a shell command, abstracting WSL usage."""
    if USE_WSL:
        return run_wsl_command(command)
    else:
        return run_linux_command(command)

print("‚úÖ Shell command helpers are defined.")

"""### 2.2. Install Dependencies"""

import sys

# Core RAG pipeline dependencies:
# - weaviate-client: connects to vector database for storing document embeddings
# - langchain: orchestrates RAG workflow (retrieval + generation)
# - langchain-openai: integrates OpenAI LLMs and embeddings into the pipeline
# - langchain-community: provides additional integrations and utilities
# - python-dotenv: loads API keys and configuration from .env file
# - pandas: processes and displays retrieved results in tabular format

# Local embedding models dependencies:
# - sentence-transformers: runs embedding models locally (alternative to OpenAI embeddings)
# - accelerate: enables GPU acceleration for faster local model inference

# HuggingFace serverless API dependencies:
# - huggingface_hub: calls HuggingFace inference API for serverless embedding generation

# Prefer install in a single command or requirements.txt to allow pip resolve dependencies better
!"{sys.executable}" -m pip install -q weaviate-client==4.18.3 langchain==1.1.2 langchain-openai==1.1.0 python-dotenv==1.2.1 pandas==2.2.2 sentence-transformers==5.1.2 accelerate==1.12.0 huggingface-hub==0.36.0 requests==2.32.4

print("‚úÖ Required libraries have been installed.")

"""## 3. Configuration

Set up the necessary configurations for Weaviate and Azure OpenAI.

**Action Required**: You must create a `.env` file in the same directory as this notebook and add your Azure OpenAI credentials.
"""

import os
from enum import StrEnum, auto
from dotenv import load_dotenv

# ==========================================
# 1. Credentials from .env file
# ==========================================

ENV_TEMPLATE = r"""
# --- SECRETS (add to .gitignore and never commit!) ---
AZURE_OPENAI_API_KEY=''
HUGGINGFACE_API_TOKEN=''
GROQ_API_KEY=''
GITHUB_MODELS_API_TOKEN=''
"""

# Create a .env file if it doesn't exist
if not os.path.exists('.env'):
    with open('.env', 'w', encoding='utf-8') as f:
        f.write(ENV_TEMPLATE)
    print("‚ö†Ô∏è Created a template .env file. Please fill it with your credentials and rerun cell.")


# Load environment variables from .env file
load_dotenv()

# ==========================================
# 2. OPTIONS: Model Providers & Registry
# ==========================================

class Provider(StrEnum):
    """Model providers. Used as options. """
    AZURE = "azure"    # models from Azure OpenAI or EPAM DIAL (requires an API key and funds in the account)
    LOCAL = "local"    # to use a local LLM via Hugging Face Pipeline (free, runs on CPU/GPU, here is CPU optimized)
    HF_API = "hf_api"  # HuggingFace API (requires an API key, some models are deployed on demand for free)
    GROQ = "groq"      # Groq API (Free Beta, requires GROQ_API_KEY, 30-60 RPM, 100-14.4k requestes per day)
    GITHUB = "github"  # GitHub Models (Free Tier, requires GITHUB_MODELS_API_TOKEN, 1-15 RPM, 8-150 per day)


# --- GLOBAL SWITCHES ---

# 1. EMBEDDINGS SOURCE
# Note: Groq does NOT support embeddings. If using Groq for LLM, use others for embeddings.
EMBEDDING_SOURCE = Provider.AZURE

# 2. LLM (CHAT) SOURCE
LLM_SOURCE = Provider.GROQ


class Models:
    """
    Registry of all available models across providers.
    Format: "Model Name/Path or Deployment Name": "tag1, tag2, type: embedding, ..."
    """
    REGISTRY = {
        # --- AZURE (DIAL) MODELS ---
        "gpt-oss-120b": "azure",  # the most pricy from this collection of five models
        "gemini-2.5-flash-lite": "azure",  # average price among 5
        "gemini-2.0-flash-lite": "azure",  # lower price among 5
        "rlab-qwq-32b": "azure", # Qwen QWQ 32B, the lowest price, reasoning, slow
        "rlab-qwen3-32b": "azure", # Qwen 3 32B, the lowest price, non-reasoning, slow
        "text-embedding-3-small-1": "azure, type: embedding",  # the cheapest from the provider

        # --- HUGGING FACE API ---
        # Note: Qwen2.5-72B is currently the best free model on HF (by the opinion of AI that is 1-year-old).
        # If "Qwen/Qwen3..." is not available, switch to "Qwen/Qwen2.5-72B-Instruct"
        "Qwen/Qwen2.5-72B-Instruct": "hf_api",
        # Standard HF embedding model, 384 dimensions, use locally if Gemma doesn't work
        "sentence-transformers/all-MiniLM-L6-v2": "hf_api, local, type: embedding",

        # --- GROQ API ---
        # Note: Groq does NOT provide embedding models.
        "groq/compound": "groq",
        "groq/compound-mini": "groq",
        "llama-3.1-70b-versatile": "groq",
        "llama-3.1-8b-instant": "groq",
        "openai/gpt-oss-120b": "groq",
        "openai/gpt-oss-20b": "groq",
        "qwen/qwen3-32b": "groq",  # 60 RPM, other 30 RPM

        # --- GITHUB MODELS ---
        "gpt-4o-mini": "github",  # "Lite" plan, 15 RPM / 150 RPD
        "text-embedding-3-small": "github, type: embedding",  # "Lite" plan

        # --- LOCAL MODELS ---
        # If you have access to Gemma (you logged in via huggingface-cli with HF API token and accepted Gemma licenses)
        "google/embeddinggemma-300m": "local, type: embedding",  #  (768 dimensions)
        "google/gemma-3-1b-it": "local",
    }

    @classmethod
    def get(cls, selector, *required_tags):
        """
        Selects model by Index (int) or Substring (str).
        Validates presence of all required tags (strings or Provider enums).
        """
        for i, (name, tags_str) in enumerate(cls.REGISTRY.items()):
            # Check 1: Selector match (Index or Substring)
            is_match = (isinstance(selector, int) and i == selector) or \
                       (isinstance(selector, str) and selector in name)

            if is_match:
                # Check 2: Validate all required tags exist in the model's tag string
                # StrEnum automatically converts to string here
                if all(str(tag) in tags_str for tag in required_tags):
                    return name

        raise StopIteration(f"Model not found or tags mismatch for: {selector}")

# ==========================================
# 3. CONFIGURATION
# ==========================================

# --- AZURE CONFIG ---
AZURE_API_KEY = os.getenv("AZURE_OPENAI_API_KEY")
AZURE_ENDPOINT = 'https://ai-proxy.lab.epam.com'
AZURE_API_VERSION = '2025-03-01-preview'

# Get full name or path or deployment name of the model
AZURE_CHAT_DEPLOYMENT = Models.get(2, Provider.AZURE)
AZURE_EMBEDDING_DEPLOYMENT = Models.get("text-embedding-3-small-1", Provider.AZURE, "type: embedding")

# --- HUGGING FACE CONFIG ---
HF_API_TOKEN = os.getenv("HUGGINGFACE_API_TOKEN")
HF_CHAT_MODEL = Models.get("Qwen2.5-72B-Instruct", Provider.HF_API)
HF_EMBEDDING_MODEL = Models.get("all-MiniLM", Provider.HF_API, "type: embedding")
HF_ENDPOINT = "https://router.huggingface.co/v1"

# --- GROQ CONFIG ---
GROQ_API_KEY = os.getenv("GROQ_API_KEY")
GROQ_CHAT_MODEL = Models.get("groq/compound-mini", Provider.GROQ)
GROQ_ENDPOINT = "https://api.groq.com/openai/v1"

# --- GITHUB MODELS CONFIG ---
GITHUB_TOKEN = os.getenv("GITHUB_MODELS_API_TOKEN")
GITHUB_CHAT_MODEL = Models.get("gpt-4o-mini", Provider.GITHUB)
GITHUB_EMBEDDING_MODEL = Models.get("text-embedding-3-small", Provider.GITHUB, "type: embedding")
GITHUB_ENDPOINT = "https://models.inference.ai.azure.com"

# --- LOCAL MODELS CONFIG ---
LOCAL_LLM_MODEL = Models.get("gemma-3-1b", Provider.LOCAL)
LOCAL_EMBEDDING_MODEL = Models.get("embeddinggemma", Provider.LOCAL, "type: embedding")

# --- VECTOR DATABASE CONFIGURATION: WEAVIATE ---
WEAVIATE_CONTAINER_NAME = "simple-rag-weaviate"
WEAVIATE_IMAGE = "semitechnologies/weaviate:1.33.7"
WEAVIATE_HTTP_PORT = 8080
WEAVIATE_GRPC_PORT = 50051

print(f"‚úÖ Configuration loaded.\n   Embeddings: {EMBEDDING_SOURCE}\n   LLM: {LLM_SOURCE}")

"""## 4. Data Generation"""

documents_data = [
    {
        "id": 1,
        "title": "Rijksmuseum",
        "category": "Museum",
        "tags": ["art", "history", "culture"],
        "content": (
            "The Rijksmuseum is the national museum of the Netherlands, located in Amsterdam. "
            "It houses a large collection of art and historical artifacts, including famous works "
            "from the Dutch Golden Age. Visitors can explore exhibitions ranging from paintings "
            "and sculptures to decorative arts and historical objects."
        )
    },
    {
        "id": 2,
        "title": "Van Gogh Museum",
        "category": "Museum",
        "tags": ["art", "painting"],
        "content": (
            "The Van Gogh Museum is dedicated to the life and work of Vincent van Gogh. "
            "It contains the largest collection of his paintings and drawings, offering insight "
            "into his artistic development and personal story."
        )
    },
    {
        "id": 3,
        "title": "Anne Frank House",
        "category": "Historic Site",
        "tags": ["history", "WWII"],
        "content": (
            "The Anne Frank House is a historic home where Anne Frank and her family hid during "
            "World War II. Today, it serves as a museum that tells the story of Anne Frank and "
            "the persecution of Jews during the war."
        )
    },
    {
        "id": 4,
        "title": "Vondelpark",
        "category": "Park",
        "tags": ["nature", "outdoor"],
        "content": (
            "Vondelpark is Amsterdam‚Äôs most popular public park. It offers green spaces, walking "
            "paths, ponds, and open-air theaters, making it a favorite place for both locals "
            "and tourists to relax."
        )
    },
    {
        "id": 5,
        "title": "Jordaan",
        "category": "Neighborhood",
        "tags": ["culture", "local"],
        "content": (
            "Jordaan is a charming neighborhood known for its narrow streets, independent shops, "
            "caf√©s, and galleries. It has a strong local character and a vibrant cultural atmosphere."
        )
    },
    {
        "id": 6,
        "title": "De Pijp",
        "category": "Neighborhood",
        "tags": ["food", "multicultural"],
        "content": (
            "De Pijp is a lively and diverse neighborhood famous for its restaurants, bars, "
            "and the Albert Cuyp Market. It attracts young people and visitors looking for an "
            "energetic urban experience."
        )
    },
    {
        "id": 7,
        "title": "Albert Cuyp Market",
        "category": "Market",
        "tags": ["food", "shopping"],
        "content": (
            "Albert Cuyp Market is the largest daily street market in Amsterdam. "
            "It offers fresh food, clothing, souvenirs, and local specialties."
        )
    },
    {
        "id": 8,
        "title": "NEMO Science Museum",
        "category": "Museum",
        "tags": ["science", "kids"],
        "content": (
            "NEMO Science Museum is an interactive museum focused on science and technology. "
            "It is especially popular with families and children due to its hands-on exhibits."
        )
    },
    {
        "id": 9,
        "title": "A'DAM Lookout",
        "category": "Attraction",
        "tags": ["view", "panorama"],
        "content": (
            "A'DAM Lookout is an observation deck that offers panoramic views of Amsterdam. "
            "It is known for its rooftop swing that extends over the edge of the building."
        )
    },
    {
        "id": 10,
        "title": "Amsterdam Light Festival",
        "category": "Event",
        "tags": ["art", "winter"],
        "content": (
            "The Amsterdam Light Festival is a recurring winter event featuring light installations "
            "created by artists from around the world. The artworks are displayed along canals "
            "and streets."
        )
    },
    {
        "id": 11,
        "title": "King's Day",
        "category": "Event",
        "tags": ["celebration", "national"],
        "content": (
            "King's Day is a national holiday celebrated annually in the Netherlands. "
            "Amsterdam hosts street markets, concerts, and boat parties throughout the city."
        )
    },
    {
        "id": 12,
        "title": "Pride Amsterdam",
        "category": "Event",
        "tags": ["equality", "festival"],
        "content": (
            "Pride Amsterdam is an annual celebration promoting diversity and inclusion. "
            "The highlight is the Canal Parade, where decorated boats move through the city‚Äôs canals."
        )
    },
    {
        "id": 13,
        "title": "Eye Film Museum",
        "category": "Museum",
        "tags": ["cinema", "culture"],
        "content": (
            "The Eye Film Museum focuses on film history and modern cinema. "
            "It hosts exhibitions, screenings, and educational programs."
        )
    },
    {
        "id": 14,
        "title": "Museumplein",
        "category": "Public Space",
        "tags": ["culture", "events"],
        "content": (
            "Museumplein is a large public square surrounded by major museums. "
            "It frequently hosts festivals, exhibitions, and public gatherings."
        )
    },
    {
        "id": 15,
        "title": "Heineken Experience",
        "category": "Attraction",
        "tags": ["beer", "history"],
        "content": (
            "The Heineken Experience is a visitor attraction located in a former brewery. "
            "It offers tours explaining the history of the Heineken brand and brewing process."
        )
    },
    {
        "id": 16,
        "title": "Rembrandt House Museum",
        "category": "Museum",
        "tags": ["art", "history"],
        "content": (
            "The Rembrandt House Museum is the former home of the artist Rembrandt van Rijn. "
            "It provides insight into his life and work through exhibitions and demonstrations."
        )
    },
    {
        "id": 17,
        "title": "Artis Zoo",
        "category": "Zoo",
        "tags": ["animals", "family"],
        "content": (
            "Artis is one of the oldest zoos in Europe. It includes a zoo, aquarium, "
            "planetarium, and botanical garden."
        )
    },
    {
        "id": 18,
        "title": "Amsterdam Canals",
        "category": "Attraction",
        "tags": ["UNESCO", "history"],
        "content": (
            "The Amsterdam canal ring is a UNESCO World Heritage Site. "
            "Canal tours offer visitors a unique view of the city‚Äôs architecture and history."
        )
    },
    {
        "id": 19,
        "title": "The Concertgebouw",
        "category": "Music Venue",
        "tags": ["classical", "concerts"],
        "content": (
            "The Concertgebouw is a world-renowned concert hall known for its exceptional "
            "acoustics and classical music performances."
        )
    },
    {
        "id": 20,
        "title": "Amsterdam Dance Event",
        "category": "Event",
        "tags": ["music", "electronic"],
        "content": (
            "Amsterdam Dance Event is a major electronic music festival and conference "
            "attracting artists, professionals, and fans from around the world."
        )
    }
]


print(f"‚úÖ Generated {len(documents_data)} documents.")

"""## 5. Environment Setup"""

!pip install -q faiss-cpu sentence-transformers python-dotenv requests
import os
from dotenv import load_dotenv
load_dotenv()  # loads .env we created above (if present)

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import pickle

# Choose a sentence-transformer model (works in Colab)
MODEL_NAME = "all-MiniLM-L6-v2"  # small & fast, 384-dim

print("Loading embedding model:", MODEL_NAME)
embedder = SentenceTransformer(MODEL_NAME)

# Example documents
documents = [
    "Weaviate is an open-source vector database that allows storing vectors.",
    "FAISS is a library for efficient similarity search and clustering of dense vectors.",
    "Groq is an accelerator/ML API (here used as an LLM provider).",
    "Sentence Transformers provide embeddings for text."
]

# Compute embeddings
embeddings = embedder.encode(documents, show_progress_bar=True)
embeddings = np.array(embeddings).astype("float32")
d = embeddings.shape[1]  # embedding dimension (e.g., 384)

# Build FAISS index
index = faiss.IndexFlatL2(d)  # simple flat index (exact search)
index.add(embeddings)
print(f"Index built. Number of vectors: {index.ntotal}")

# Save index + metadata if you want
faiss.write_index(index, "faiss_index.idx")
with open("docs.pkl", "wb") as f:
    pickle.dump(documents, f)

print("Saved faiss_index.idx and docs.pkl (runtime filesystem).")

import faiss, pickle, numpy as np
from sentence_transformers import SentenceTransformer

# Load if you restarted the runtime (optional)
# index = faiss.read_index("faiss_index.idx")
# with open("docs.pkl", "rb") as f:
#     documents = pickle.load(f)

# We'll reuse embedder from previous cell or reload:
embedder = SentenceTransformer("all-MiniLM-L6-v2")

def search(query, top_k=3):
    q_emb = embedder.encode([query]).astype("float32")
    D, I = index.search(q_emb, top_k)
    results = []
    for idx, dist in zip(I[0], D[0]):
        results.append({"doc": documents[idx], "score": float(dist)})
    return results

# Demo
print(search("What is FAISS used for?", top_k=2))

"""## 6. Embeddings and Data Ingestion

### 6.1 Models API Clients
"""

from langchain_openai import AzureOpenAIEmbeddings, AzureChatOpenAI, ChatOpenAI, OpenAIEmbeddings
from huggingface_hub import InferenceClient
from langchain_core.messages import AIMessage
from langchain_core.runnables import Runnable, RunnableConfig
from sentence_transformers import SentenceTransformer
from transformers import pipeline
import torch
import time
import os

# --- Wrapper for Rate Limiting ---
class RateLimitedRunnable(Runnable):
    """
    Wraps a Runnable to enforce a minimum time interval between calls.
    Useful for free tier APIs with strict rate limits (RPM).
    """

    def __init__(self, runnable, min_interval_seconds):
        self.runnable = runnable
        self.min_interval = min_interval_seconds
        self.last_call_time = 0

    def invoke(self, input, config: RunnableConfig = None, **kwargs):
        elapsed = time.time() - self.last_call_time
        if elapsed < self.min_interval:
            sleep_time = self.min_interval - elapsed
            # print(f"‚è≥ Rate limit: sleeping for {sleep_time:.2f}s...")
            time.sleep(sleep_time)

        result = self.runnable.invoke(input, config, **kwargs)
        self.last_call_time = time.time()
        return result


# --- Wrapper class for the local Embeddings model ---
class LocalHuggingFaceEmbeddings:
    """
    This class adapts a local SentenceTransformer model
    to the LangChain interface, which expects the methods embed_documents and embed_query.
    """

    def __init__(self, model_name):
        print(f"üì• Loading local embedding model: {model_name}...")
        try:
            self.model = SentenceTransformer(model_name)
            print("‚úÖ Local embedding model loaded successfully.")
        except Exception as e:
            print(f"‚ùå Error loading {model_name}. Falling back to 'all-MiniLM-L6-v2'.")
            print(f"Error details: {e}")
            self.model = SentenceTransformer("sentence-transformers/all-MiniLM-L6-v2")

    def embed_documents(self, texts):
        # Returns a list of lists
        embeddings = self.model.encode(texts, convert_to_numpy=True)
        return embeddings.tolist()

    def embed_query(self, text):
        # Returns a single list
        embedding = self.model.encode(text, convert_to_numpy=True)
        return embedding.tolist()


# --- Wrapper class for the local LLM ---
class LocalHuggingFaceChatModel(Runnable):
    """
    A simple wrapper around the Transformers Pipeline to make it compatible
    with LangChain's 'invoke' method and the pipe '|' operator.
    """

    def __init__(self, model_name):
        print(f"üì• Loading local LLM: {model_name}...")
        # This is the 'Automatic Transmission' setup we discussed:
        # 1. device=-1 forces CPU usage.
        # 2. torch_dtype=torch.float32 is the fastest format for CPU.
        self.pipe = pipeline(
            "text-generation",
            model=model_name,
            device=-1,  # CPU
            torch_dtype=torch.float32
        )
        print("‚úÖ Local LLM loaded successfully.")

    def invoke(self, input_data, config: RunnableConfig = None, **kwargs):
        """
        Adapts LangChain inputs (PromptValue or Messages) to the pipeline format.
        """
        # 1. Convert LangChain input to the list-of-dicts format expected by the pipeline
        messages = []

        # Handle LangChain PromptValue (which has .to_messages())
        if hasattr(input_data, 'to_messages'):
            lc_messages = input_data.to_messages()
            for msg in lc_messages:
                # Map LangChain message types to role strings
                role = "user"
                if msg.type == "system":
                    role = "system"
                elif msg.type == "ai":
                    role = "assistant"

                # Gemma pipeline expects content as a list of dicts or string.
                messages.append({"role": role, "content": [{"type": "text", "text": msg.content}]})

        # Handle raw string input (fallback)
        elif isinstance(input_data, str):
            messages = [{"role": "user", "content": [{"type": "text", "text": input_data}]}]

        # 2. Run the pipeline ("Automatic Transmission")
        # We set max_new_tokens to limit the answer length
        outputs = self.pipe(messages, max_new_tokens=512)

        # 3. Extract the generated text
        # The pipeline returns a list of dicts. The last message is the assistant's reply.
        generated_text = outputs[0]['generated_text'][-1]['content']

        # 4. Return as an AIMessage to satisfy LangChain's StrOutputParser
        return AIMessage(content=generated_text)


class HFServerlessEmbeddings:
    """Wrapper around InferenceClient for LangChain compatibility"""

    def __init__(self, model_name, api_key):
        self.client = InferenceClient(token=api_key)
        self.model_name = model_name

    def embed_documents(self, texts):
        # Use feature_extraction
        result = self.client.feature_extraction(texts, model=self.model_name)
        return result.tolist()  # array to list

    def embed_query(self, text):
        # For single query
        result = self.client.feature_extraction([text], model=self.model_name)
        return result.tolist()[0]


# --- 1. Setup LangChain Clients ---
print("--- 1. Setting up AI clients ---")
try:
    # --- EMBEDDING MODEL SETUP ---
    if EMBEDDING_SOURCE == Provider.LOCAL:
        embeddings_model = LocalHuggingFaceEmbeddings(LOCAL_EMBEDDING_MODEL)
    elif EMBEDDING_SOURCE == Provider.HF_API:
        print(f"üåê Connecting to HF Serverless Embeddings: {HF_EMBEDDING_MODEL}")
        embeddings_model = HFServerlessEmbeddings(
            model_name=HF_EMBEDDING_MODEL,
            api_key=HF_API_TOKEN
        )
    elif EMBEDDING_SOURCE == Provider.GITHUB:
        print(f"üåê Connecting to GitHub Models Embeddings: {GITHUB_EMBEDDING_MODEL}")
        # GitHub Models uses standard OpenAI client structure
        embeddings_model = OpenAIEmbeddings(
            model=GITHUB_EMBEDDING_MODEL,
            openai_api_key=GITHUB_TOKEN,
            openai_api_base=GITHUB_ENDPOINT,
            check_embedding_ctx_length=False  # Disable check as it requires encoding files
        )
    else:  # Azure (DIAL)
        embeddings_model = AzureOpenAIEmbeddings(
            azure_deployment=AZURE_EMBEDDING_DEPLOYMENT,
            openai_api_version=AZURE_API_VERSION,
            azure_endpoint=AZURE_ENDPOINT,
            api_key=AZURE_API_KEY,
            dimensions=256  # size of embedding vectors, default is 1536
        )

    # --- CHAT MODEL SETUP ---
    if LLM_SOURCE == Provider.LOCAL:
        chat_model = LocalHuggingFaceChatModel(LOCAL_LLM_MODEL)
    elif LLM_SOURCE == Provider.HF_API:
        print(f"üåê Connecting to HF Serverless Chat: {HF_CHAT_MODEL}")
        # Using standard OpenAI client but pointing to HF URL
        chat_model = ChatOpenAI(
            model=HF_CHAT_MODEL,
            openai_api_key=HF_API_TOKEN,
            openai_api_base=HF_ENDPOINT,
            temperature=0.1,
            max_tokens=1024
        )
    elif LLM_SOURCE == Provider.GROQ:
        print(f"üåê Connecting to Groq Chat: {GROQ_CHAT_MODEL}")
        # Groq is OpenAI-compatible.
        # Hardcoded limit: 30 RPM -> 1 request every 2 seconds.
        # Note: Some models allow 60 RPM, but we stick to safe 30.
        raw_chat_model = ChatOpenAI(
            model=GROQ_CHAT_MODEL,
            openai_api_key=GROQ_API_KEY,
            openai_api_base=GROQ_ENDPOINT,
            temperature=0.1
        )
        chat_model = RateLimitedRunnable(raw_chat_model, min_interval_seconds=2.0)

    elif LLM_SOURCE == Provider.GITHUB:
        print(f"üåê Connecting to GitHub Models Chat: {GITHUB_CHAT_MODEL}")
        # GitHub Models is OpenAI-compatible.
        # Hardcoded limit: 15 RPM -> 1 request every 4 seconds.
        raw_chat_model = ChatOpenAI(
            model=GITHUB_CHAT_MODEL,
            openai_api_key=GITHUB_TOKEN,
            openai_api_base=GITHUB_ENDPOINT,
            temperature=0.1
        )
        chat_model = RateLimitedRunnable(raw_chat_model, min_interval_seconds=4.0)

    else:  # Azure (DIAL)
        chat_model = AzureChatOpenAI(
            azure_deployment=AZURE_CHAT_DEPLOYMENT,
            openai_api_version=AZURE_API_VERSION,
            azure_endpoint=AZURE_ENDPOINT,
            api_key=AZURE_API_KEY,
            temperature=0
        )
    print("‚úÖ AI clients initialized.")

except Exception as e:
    print(f"‚ùå Failed to initialize AI clients. Please check your .env file or model names. Error: {e}")
    # Stop execution if clients fail to initialize
    raise

"""### 6.2-6.5 Database Filling-up"""

!pip install -q sentence-transformers faiss-cpu tqdm python-dotenv

from sentence_transformers import SentenceTransformer
import numpy as np
from tqdm import tqdm
import math

# Choose a small, fast model
MODEL_NAME = "all-MiniLM-L6-v2"  # 384-dim, excellent for Colab experiments

print("Loading local embedding model:", MODEL_NAME)
embedder = SentenceTransformer(MODEL_NAME)

def embed_documents_local(docs, batch_size=64):
    """
    Encode documents in batches locally using sentence-transformers.
    Returns list of float32 vectors.
    """
    vectors = []
    n = len(docs)
    for i in tqdm(range(0, n, batch_size), desc="Embedding batches"):
        batch = docs[i : i + batch_size]
        emb = embedder.encode(batch, show_progress_bar=False, convert_to_numpy=True)
        # ensure float32 for FAISS / weaviate
        emb = np.array(emb, dtype="float32")
        vectors.append(emb)
    return np.vstack(vectors)

# Example usage:
# documents_data is assumed to be a list of dicts with 'content' key
contents_to_embed = [doc['content'] for doc in documents_data]
vector_embeddings = embed_documents_local(contents_to_embed, batch_size=64)
print("‚úÖ Generated embeddings:", vector_embeddings.shape)
# attach to documents
for i, doc in enumerate(documents_data):
    doc['content_vector'] = vector_embeddings[i].tolist()  # or keep as np array

"""## 7. RAG Experiment"""

# Build FAISS index
def build_faiss_index(vectors):
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    norms[norms == 0] = 1.0
    vectors_norm = vectors / norms
    faiss_idx = faiss.IndexFlatIP(vectors_norm.shape[1])  # inner-product == cosine
    faiss_idx.add(vectors_norm)
    return faiss_idx, vectors_norm

faiss_index, vectors_norm = build_faiss_index(vector_embeddings)
print("FAISS index built. Total vectors indexed:", faiss_index.ntotal)

!pip install -q sentence-transformers faiss-cpu gradio pandas tqdm

import numpy as np
import pandas as pd
import faiss
import gradio as gr

from sentence_transformers import SentenceTransformer
from tqdm import tqdm
from textwrap import shorten

EMBED_MODEL = "all-MiniLM-L6-v2"
embedder = SentenceTransformer(EMBED_MODEL)

texts = [doc["content"] for doc in documents_data]

embeddings = embedder.encode(texts, convert_to_numpy=True, show_progress_bar=True)
embeddings = embeddings.astype("float32")

# Normalize for cosine similarity
embeddings /= np.linalg.norm(embeddings, axis=1, keepdims=True)

index = faiss.IndexFlatIP(embeddings.shape[1])
index.add(embeddings)

print("Documents indexed:", index.ntotal)

def retrieve_documents(query, top_k=3):
    q_emb = embedder.encode([query], convert_to_numpy=True).astype("float32")
    q_emb /= np.linalg.norm(q_emb, axis=1, keepdims=True)

    scores, indices = index.search(q_emb, top_k)

    results = []
    for idx, score in zip(indices[0], scores[0]):
        doc = documents_data[idx]
        results.append({
            "title": doc["title"],
            "topic": doc["topic"],
            "score": float(score),
            "content": doc["content"]
        })
    return results

def generate_answer(context_docs, question):
    if not context_docs:
        return "The provided context does not contain the answer to this question."

    keywords = set(w.lower() for w in question.split() if len(w) > 3)

    sentences = []
    for doc in context_docs:
        for s in doc["content"].split("."):
            tokens = set(w.lower() for w in s.split())
            overlap = len(tokens & keywords)
            if overlap > 0:
                sentences.append((overlap, s.strip()))

    if not sentences:
        return "The provided context does not contain the answer to this question."

    sentences.sort(reverse=True)
    return "\n".join(f"- {shorten(s, 300)}" for _, s in sentences[:5])

# def rag_pipeline(user_question):
#     retrieved = retrieve_documents(user_question, top_k=3)

#     answer = generate_answer(retrieved, user_question)

#     table = pd.DataFrame([{
#         "Title": d["title"],
#         "Topic": d["topic"],
#         "Similarity": round(d["score"], 3)
#     } for d in retrieved])

#     return answer, table
def rag_pipeline(question: str):
    try:
        # 1. Expand query (or just use question)
        expanded_query = expand_query(question)

        # 2. Retrieve documents
        retrieved = faiss_search(expanded_query, top_k=5)

        # 3. Build context
        context = "\n\n---\n\n".join([r["content"] for r in retrieved])

        # 4. Generate answer
        answer = generate_answer(context, question)

        # 5. Build table for Gradio
        table_data = pd.DataFrame([
            {
                "Title": r["title"],
                "Category": r.get("category", "N/A"),
                "Score": round(r["score"], 4)
            }
            for r in retrieved
        ])

        return answer, table_data

    except Exception as e:
        # VERY IMPORTANT for debugging
        return f"‚ùå Error occurred:\n\n```\n{str(e)}\n```", pd.DataFrame()

EMBED_MODEL_NAME = "all-MiniLM-L6-v2"  # compact & fast; 384-dim

print(f"\nLoading embedding model: {EMBED_MODEL_NAME} ...")
embedder = SentenceTransformer(EMBED_MODEL_NAME)

# Embed documents in batches to avoid OOM and timeouts
def embed_documents_local(texts, batch_size=64):
    vectors = []
    for i in tqdm(range(0, len(texts), batch_size), desc="Embedding docs"):
        batch = texts[i:i+batch_size]
        emb = embedder.encode(batch, convert_to_numpy=True, show_progress_bar=False)
        vectors.append(emb.astype("float32"))
    return np.vstack(vectors)

contents_to_embed = [d["content"] for d in documents_data]
vector_embeddings = embed_documents_local(contents_to_embed, batch_size=64)
dim = vector_embeddings.shape[1]
print(f"Embeddings generated. Count={vector_embeddings.shape[0]}, dim={dim}")

# Build FAISS index (flat L2; cosine distance can be approximated by normalizing vectors)
# We'll normalize for cosine similarity search.
def build_faiss_index(vectors):
    # Normalize vectors for cosine similarity search using L2-index
    norms = np.linalg.norm(vectors, axis=1, keepdims=True)
    norms[norms == 0] = 1.0
    vectors_norm = vectors / norms
    idx = faiss.IndexFlatIP(vectors_norm.shape[1])  # inner-product on normalized vectors == cosine
    idx.add(vectors_norm)
    return idx, vectors_norm

index, vectors_norm = build_faiss_index(vector_embeddings)
print("FAISS index built. Total vectors indexed:", index.ntotal)

# Mapping index -> document
idx_to_doc = documents_data  # same order

# -------------------------
# 4) RAG utility functions
# -------------------------
def faiss_search(query_text, top_k=5):
    q_emb = embedder.encode([query_text], convert_to_numpy=True).astype("float32")
    q_emb = q_emb / (np.linalg.norm(q_emb, axis=1, keepdims=True) + 1e-12)
    D, I = faiss_index.search(q_emb, top_k)
    scores = D[0].tolist()
    indices = I[0].tolist()
    results = []
    for idx, score in zip(indices, scores):
        doc = idx_to_doc[idx]
        distance = round(1.0 - float(score), 6)  # pseudo-distance for compatibility
        results.append({"index": idx, "score": float(score), "distance": distance,
                        "title": doc["title"], "content": doc["content"], "topic": doc.get("topic")})
    return results


# Simple deterministic query expander (fallback). If GROQ API is available we prefer using it.
def expand_query_local(query_text):
    # Very lightweight "expansion": add intent tokens and ask for context-rich wording
    return f"detailed search: {query_text} include names, dates, places, and specific technical terms"

def call_groq_on_text(prompt, model="groq/compound-mini", timeout=30.0):
    if not GROQ_API_KEY:
        raise RuntimeError("GROQ_API_KEY not set")
    headers = {"Authorization": f"Bearer {GROQ_API_KEY}", "Content-Type": "application/json"}
    payload = {"model": model, "prompt": prompt, "max_tokens": 300}
    # basic call - adapt according to Groq official docs if needed (this is a common completions style)
    r = httpx.post(f"{GROQ_ENDPOINT}/completions", headers=headers, json=payload, timeout=timeout)
    r.raise_for_status()
    return r.json()

def expand_query(query_text):
    if GROQ_API_KEY:
        # attempt model-based rephrasing; fallback to local if error
        try:
            prompt = f"Rephrase the following user query into a detailed search query suitable for vector retrieval. Return ONLY the rephrased query.\n\nOriginal Query: {query_text}\n\nRephrased Query:"
            resp = call_groq_on_text(prompt, model="groq/compound-mini", timeout=30.0)
            # extract text - depends on Groq response format; try common locations
            if isinstance(resp, dict):
                # try to find text in common keys
                text = resp.get("choices", [{}])[0].get("text") or resp.get("output") or resp.get("completion") or json.dumps(resp)
            else:
                text = str(resp)
            return text.strip()[:2000]
        except Exception as e:
            print("Groq expansion failed, using local fallback. Error:", str(e))
            return expand_query_local(query_text)
    else:
        return expand_query_local(query_text)

# Answer generation: if Groq key available, call it. Otherwise do deterministic extraction from context.
def generate_answer(context, question):
    # If no context, return sentinel phrase as in your original template
    if not context.strip():
        return "The provided context does not contain the answer to this question."

    if GROQ_API_KEY:
        try:
            prompt = (
                "You are a factual assistant. Answer the user's question using ONLY the provided context.\n"
                "If the context doesn't contain the answer, say exactly: 'The provided context does not contain the answer to this question.'\n\n"
                f"Context:\n{context}\n\nQuestion: {question}\n\nAnswer (concise bullet points):"
            )
            resp = call_groq_on_text(prompt, model="groq/compound-mini", timeout=30.0)
            # parse text similar to above
            if isinstance(resp, dict):
                text = resp.get("choices", [{}])[0].get("text") or resp.get("output") or resp.get("completion") or json.dumps(resp)
            else:
                text = str(resp)
            return text.strip()
        except Exception as e:
            print("Groq call failed, falling back to local summarizer. Error:", e)

    # Local deterministic summarizer: find sentences in context that overlap with question keywords
    q_tokens = set([w.lower().strip(".,?") for w in question.split() if len(w) > 2])
    sentences = []
    # split context into sentences by simple punctuation split (keeps it lightweight)
    for s in context.split("\n"):
        s_clean = s.strip()
        if not s_clean:
            continue
        tokens = set([t.lower().strip(".,?") for t in s_clean.split() if len(t) > 2])
        overlap = len(q_tokens & tokens)
        sentences.append((overlap, s_clean))
    # sort by overlap desc and pick top 4
    sentences.sort(key=lambda x: x[0], reverse=True)
    top_sentences = [s for score, s in sentences if score > 0][:6]
    if not top_sentences:
        # no matching sentence: return not-found phrase
        return "The provided context does not contain the answer to this question."
    # Return bullet points
    bullets = "\n".join([f"- {shorten(s, width=400, placeholder='...')}" for s in top_sentences])
    return bullets

AMSTERDAM_QUESTIONS = [
    "What is the Rijksmuseum and what can visitors see there?",
    "Which museums in Amsterdam are suitable for children?",
    "What are the most important cultural events in Amsterdam?",
    "What is special about the Amsterdam canal ring?",
    "Which neighborhoods are popular with locals and why?",
    "What happens during King's Day in Amsterdam?",
    "What attractions offer panoramic views of the city?",
    "What places in Amsterdam are related to famous artists?",
    "What outdoor places can people relax in Amsterdam?",
    "What is Amsterdam Dance Event and who is it for?"
]

with gr.Blocks(title="RAG Demo (FAISS + SentenceTransformers)") as demo:
    gr.Markdown("# üîé Retrieval-Augmented Generation Demo")
    gr.Markdown("Ask a question and see how RAG retrieves documents and generates an answer.")

    question = gr.Textbox(label="Your Question", placeholder="What is the Rijksmuseum and what can visitors see there?")
    answer = gr.Markdown(label="Answer")
    retrieved_table = gr.Dataframe(label="Retrieved Documents")

    btn = gr.Button("Ask")

    btn.click(
        fn=rag_pipeline,
        inputs=question,
        outputs=[answer, retrieved_table]
    )

demo.launch()